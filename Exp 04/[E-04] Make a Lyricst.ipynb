{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "limiting-public",
   "metadata": {},
   "source": [
    "# 4-7. 프로젝트 : 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-reason",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드\n",
    "\n",
    "이미 실습(1) 데이터 다듬기에서 Cloud shell에\n",
    "\n",
    "심볼릭 링크로 ~/aiffel/lyricist/data를 생성하셨다면,\n",
    "\n",
    "~/aiffel/lyricist/data/lyrics에 데이터가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-offense",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "political-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 import 하기 \n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alleged-punch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "# print(\"Examples:\\n\", raw_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sorted-dollar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hook]\n",
      "I've been down so long, it look like up to me\n",
      "They look up to me\n",
      "I got fake people showin' fake love to me\n",
      "Straight up to my face, straight up to my face\n",
      "I've been down so long, it look like up to me\n",
      "They look up to me\n",
      "I got fake people showin' fake love to me\n",
      "Straight up to my face, straight up to my face [Verse 1]\n",
      "Somethin' ain't right when we talkin'\n"
     ]
    }
   ],
   "source": [
    "# 공백인 문장은 길이를 검사하여 길이가 0이라면 제외를 시켜 봅니다.\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus): # enumerate 열거하다 라는 뜻으로, 리스트가 있는 경우 순서와 리스트의 값을 전달하는 기능\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-discretion",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-fiber",
   "metadata": {},
   "source": [
    "## 문장 전처리\n",
    "\n",
    "### 1. 입력된 문장을 정규 표현식을 이용해서 정제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fluid-frontier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 정규표현식으로 문장이 어떻게 필터링되는지 확인해 봅니다.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-therapist",
   "metadata": {},
   "source": [
    "### 2. preprocess_sentence로 정제한 문장을 모음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "disabled-drill",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> i ve been down so long , it look like up to me <end>',\n",
       " '<start> they look up to me <end>',\n",
       " '<start> i got fake people showin fake love to me <end>',\n",
       " '<start> straight up to my face , straight up to my face <end>',\n",
       " '<start> i ve been down so long , it look like up to me <end>',\n",
       " '<start> they look up to me <end>',\n",
       " '<start> i got fake people showin fake love to me <end>',\n",
       " '<start> somethin ain t right when we talkin <end>',\n",
       " '<start> somethin ain t right when we talkin <end>',\n",
       " '<start> look like you hidin your problems <end>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue  # 길이가 0인 빈 문장은 건너뜁니다\n",
    "    if sentence[-1] == \"]\": continue # ']'로 끝나는 파트를 나눈 문장은 건너뜁니다\n",
    "    if sentence[-1] == \")\": continue # ')'로 끝나는 코러스 문장은 건너뜁니다    \n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if preprocessed_sentence.count(\" \") >= 15:\n",
    "            continue\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-method",
   "metadata": {},
   "source": [
    "### 3. tokenize() 함수로 데이터를 Tensor로 변환 (Tokenizer와 pad_sequences를 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "utility-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    # maxlen=15로 설정\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fuzzy-alliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    4   92 ...   10   12    3]\n",
      " [   2   37  134 ...    0    0    0]\n",
      " [   2    4   36 ...    0    0    0]\n",
      " ...\n",
      " [   2   87  694 ...    0    0    0]\n",
      " [   2  204    3 ...    0    0    0]\n",
      " [   2    9 1525 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7febb8280e90>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(148425, 15)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor, tokenizer = tokenize(corpus) # 토큰화 적용\n",
    "\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "boring-witness",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    4   92  104   58   31  164    5   11  134]\n",
      " [   2   37  134   30   10   12    3    0    0    0]\n",
      " [   2    4   36  798  170 2348  798   38   10   12]]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력\n",
    "\n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "altered-healing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "# tokenizer에 구축된 단어 사전 확인\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break # 단어 사전의 10번째 단어까지 출력\n",
    "        \n",
    "print(tokenizer.num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-buffalo",
   "metadata": {},
   "source": [
    "#### Source & Target\n",
    "\n",
    "모델을 학습할 때 입력값(source)과 출력값(target)을 데이터셋 형식으로 정의합니다\n",
    "\n",
    "- 입력값(source) : Start를 시작으로 맨 뒤값인 End를 제외한 문장(== sentence[:-1])\n",
    "- 출력값(target) : Start 다음 단어를 시작으로 End까지의 문장(== sentecne[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bottom-spirit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   4  92 104  58  31 164   5  11 134  23  30  10  12]\n",
      "[  4  92 104  58  31 164   5  11 134  23  30  10  12   3]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높으므로, 잘라서 소스 문장을 생성합니다\n",
    "src_input = tensor[:, :-1]\n",
    "\n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-department",
   "metadata": {},
   "source": [
    "## Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-submission",
   "metadata": {},
   "source": [
    "### 1. sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "partial-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷 런 패키지를 이용해서 위에서 train과 target을 train과 validation 셋으로 분리를 해줍니다. 8:2로 분리를 하겠습니다.\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "flush-biodiversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (118740, 14)\n",
      "Target Train: (118740, 14)\n"
     ]
    }
   ],
   "source": [
    "# train의 맨 뒷 단어를 뺀 문장을 분석해서 앞 단어가 없는 target을 찾는것이 목적이기 때문에 shape는 동일한 모습을 하고있습니다.\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-omega",
   "metadata": {},
   "source": [
    "### 2. 데이터셋 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "protective-offset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input) # 텐서의 1차원, 전체 문장의 개수\n",
    "BATCH_SIZE = 256 # 문장의 개수\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "# tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-extent",
   "metadata": {},
   "source": [
    "## Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-viking",
   "metadata": {},
   "source": [
    "### 1. 모델 구조 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "municipal-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-contact",
   "metadata": {},
   "source": [
    "### 2. 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-sleeve",
   "metadata": {},
   "source": [
    "## - 하이퍼 파라미터 지정과 모델 생성\n",
    "    - embedding_size를 512으로 올려 고려하는 feature의 수를 늘림\n",
    "    - hidden_size를 2048으로 올려 각 레이어의 노드 수를 늘림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "attempted-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512 # 단어 하나의 특징 수\n",
    "hidden_size = 2048 # 퍼셉트론의 갯수\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "distinguished-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "for src_sample, tgt_sample in dataset.take(1): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "civilian-assistant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 1.46774313e-04, -4.96039866e-04,  2.59472090e-05, ...,\n",
       "          1.27862237e-04,  9.56478834e-05,  2.65176524e-04],\n",
       "        [-1.50246036e-04, -3.84326471e-04, -5.37159176e-05, ...,\n",
       "          1.42909921e-04,  1.60650146e-04,  3.43132153e-04],\n",
       "        [-2.00931652e-04,  2.80696240e-05, -9.22709951e-05, ...,\n",
       "          2.37725006e-04,  2.54640589e-04,  2.63643131e-04],\n",
       "        ...,\n",
       "        [-2.09589000e-03,  1.08527229e-03,  3.34497483e-04, ...,\n",
       "         -1.03862677e-03, -5.92302822e-04, -3.37539939e-04],\n",
       "        [-1.95793086e-03,  1.35472941e-03,  4.05756466e-04, ...,\n",
       "         -1.50318746e-03, -8.96097510e-04, -6.74814393e-04],\n",
       "        [-1.78136653e-03,  1.58377504e-03,  4.36176226e-04, ...,\n",
       "         -1.92987768e-03, -1.19189301e-03, -9.71210888e-04]],\n",
       "\n",
       "       [[ 1.46774313e-04, -4.96039866e-04,  2.59472090e-05, ...,\n",
       "          1.27862237e-04,  9.56478834e-05,  2.65176524e-04],\n",
       "        [-1.87158133e-04, -5.12017112e-04,  2.73497571e-04, ...,\n",
       "          1.66229380e-04,  1.39870230e-04,  5.91126445e-04],\n",
       "        [-4.13458823e-04, -8.34700477e-04,  8.67003764e-05, ...,\n",
       "         -1.71458614e-05,  1.32040164e-04,  7.90604216e-04],\n",
       "        ...,\n",
       "        [-6.80908212e-04,  8.01252550e-04,  1.11553025e-04, ...,\n",
       "         -2.32488243e-03,  5.50383993e-04, -1.39131429e-04],\n",
       "        [-6.74284704e-04,  1.25083292e-03,  1.49531930e-04, ...,\n",
       "         -2.60835909e-03,  1.35369759e-04, -4.98050998e-04],\n",
       "        [-6.30073831e-04,  1.61520671e-03,  1.73989538e-04, ...,\n",
       "         -2.86895991e-03, -2.77020852e-04, -8.26099480e-04]],\n",
       "\n",
       "       [[ 1.46774313e-04, -4.96039866e-04,  2.59472090e-05, ...,\n",
       "          1.27862237e-04,  9.56478834e-05,  2.65176524e-04],\n",
       "        [ 5.59653890e-05, -6.81839942e-04,  1.73462773e-04, ...,\n",
       "          6.85449399e-04, -2.85805349e-04,  4.31828259e-04],\n",
       "        [ 3.05873109e-04, -5.00680471e-04,  4.24126512e-04, ...,\n",
       "          1.36539305e-03, -7.05421146e-04,  7.80834642e-04],\n",
       "        ...,\n",
       "        [-8.20351590e-04,  1.15039665e-03, -1.31548644e-04, ...,\n",
       "         -7.58306007e-04, -1.27355347e-03, -3.76609940e-04],\n",
       "        [-8.04922485e-04,  1.41517515e-03, -1.36268529e-04, ...,\n",
       "         -1.27438421e-03, -1.54363224e-03, -7.09792192e-04],\n",
       "        [-7.60612427e-04,  1.63909153e-03, -1.49957035e-04, ...,\n",
       "         -1.73062249e-03, -1.78006012e-03, -9.99502954e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.46774313e-04, -4.96039866e-04,  2.59472090e-05, ...,\n",
       "          1.27862237e-04,  9.56478834e-05,  2.65176524e-04],\n",
       "        [-2.11595616e-04, -5.64502727e-04,  8.10279089e-05, ...,\n",
       "          3.23503482e-04,  4.80141200e-04,  4.14629991e-04],\n",
       "        [-6.25009532e-04, -1.06006933e-04,  8.08097830e-05, ...,\n",
       "          6.38884841e-04,  8.34264967e-04,  6.71072165e-04],\n",
       "        ...,\n",
       "        [-6.27635920e-04,  4.65290883e-04,  1.75914518e-03, ...,\n",
       "         -7.34498753e-05,  9.56301577e-04,  2.15928725e-04],\n",
       "        [-8.18306988e-04,  5.66843664e-04,  1.76049559e-03, ...,\n",
       "         -1.63559060e-04,  7.51667132e-04,  1.71471402e-04],\n",
       "        [-1.00603583e-03,  7.96647859e-04,  1.72416936e-03, ...,\n",
       "         -3.31148156e-04,  4.97017754e-04, -3.50218506e-05]],\n",
       "\n",
       "       [[ 1.46774313e-04, -4.96039866e-04,  2.59472090e-05, ...,\n",
       "          1.27862237e-04,  9.56478834e-05,  2.65176524e-04],\n",
       "        [ 1.72411164e-05, -8.42465670e-04, -1.49529122e-04, ...,\n",
       "          6.17749538e-05,  4.42238124e-05,  5.01895789e-04],\n",
       "        [ 1.37448209e-04, -1.00783061e-03,  8.38802298e-05, ...,\n",
       "          3.34224023e-04,  9.55929063e-05,  4.83812619e-04],\n",
       "        ...,\n",
       "        [-7.47468614e-04,  1.57334120e-03,  6.55429845e-04, ...,\n",
       "         -1.81611243e-03, -6.02261047e-04, -1.39450573e-03],\n",
       "        [-6.66915672e-04,  1.81998708e-03,  6.56803546e-04, ...,\n",
       "         -2.19561812e-03, -9.62221413e-04, -1.67449750e-03],\n",
       "        [-5.73727651e-04,  2.01573572e-03,  6.24108710e-04, ...,\n",
       "         -2.51904572e-03, -1.28974440e-03, -1.89659023e-03]],\n",
       "\n",
       "       [[ 1.46774313e-04, -4.96039866e-04,  2.59472090e-05, ...,\n",
       "          1.27862237e-04,  9.56478834e-05,  2.65176524e-04],\n",
       "        [ 2.72591278e-04, -1.01096579e-03, -6.29507995e-05, ...,\n",
       "          1.93068292e-04, -1.98008929e-05,  3.28038062e-04],\n",
       "        [ 4.38957824e-04, -1.11724215e-03, -7.01057434e-05, ...,\n",
       "          7.40161486e-05, -3.22160515e-04,  4.08466294e-04],\n",
       "        ...,\n",
       "        [ 5.03092422e-04, -4.83985437e-04,  1.78726346e-04, ...,\n",
       "         -1.31982626e-04, -5.17552719e-04,  5.32838167e-05],\n",
       "        [ 3.09165654e-04,  7.70419865e-05,  3.07656068e-04, ...,\n",
       "         -5.34423278e-04, -7.97117187e-04, -2.83287023e-04],\n",
       "        [ 1.57188784e-04,  5.63114882e-04,  4.07433312e-04, ...,\n",
       "         -9.65274288e-04, -1.09972735e-03, -6.09295384e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-relation",
   "metadata": {},
   "source": [
    "- 생성된 모델 살펴보기: model_summary\n",
    "    - 하나의 배치만 이용하면 임시 모델을 생성해볼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "minimal-beginning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  6144512   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  20979712  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  24590049  \n",
      "=================================================================\n",
      "Total params: 85,276,897\n",
      "Trainable params: 85,276,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fitted-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-december",
   "metadata": {},
   "source": [
    "### 3. 모델 학습: fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "professional-provincial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "464/464 [==============================] - 461s 986ms/step - loss: 3.8308 - val_loss: 2.9309\n",
      "Epoch 2/10\n",
      "464/464 [==============================] - 458s 988ms/step - loss: 2.8112 - val_loss: 2.6885\n",
      "Epoch 3/10\n",
      "464/464 [==============================] - 458s 988ms/step - loss: 2.4669 - val_loss: 2.5118\n",
      "Epoch 4/10\n",
      "464/464 [==============================] - 459s 990ms/step - loss: 2.1527 - val_loss: 2.3791\n",
      "Epoch 5/10\n",
      "464/464 [==============================] - 457s 985ms/step - loss: 1.8497 - val_loss: 2.2797\n",
      "Epoch 6/10\n",
      "464/464 [==============================] - 457s 985ms/step - loss: 1.5783 - val_loss: 2.2077\n",
      "Epoch 7/10\n",
      "464/464 [==============================] - 458s 987ms/step - loss: 1.3557 - val_loss: 2.1738\n",
      "Epoch 8/10\n",
      "464/464 [==============================] - 457s 984ms/step - loss: 1.1895 - val_loss: 2.1675\n",
      "Epoch 9/10\n",
      "464/464 [==============================] - 457s 985ms/step - loss: 1.0747 - val_loss: 2.1798\n",
      "Epoch 10/10\n",
      "464/464 [==============================] - 458s 987ms/step - loss: 1.0081 - val_loss: 2.2018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7febbed7f190>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(enc_train, dec_train, validation_data=(enc_val, dec_val),epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-atmosphere",
   "metadata": {},
   "source": [
    "### 4. 실습 : 잘 만들어졌는지 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "contemporary-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "separated-threshold",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m not gonna crack <end> '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-complex",
   "metadata": {},
   "source": [
    "# Step 6. 평가 및 회고\n",
    "\n",
    "## 평가\n",
    "\n",
    "### 1. 가사 텍스트 생성 모델이 정상적으로 동작하는가?\n",
    "\n",
    "- 텍스트 제너레이션 결과가 그럴듯한 문장으로 생성되는가?\n",
    "\n",
    "- 모델 학습 후 실습에서 문장을 생성해본 결과 적절한 가사 텍스트를 생성하였음을 확인할 수 있습니다.\n",
    "   - i love -> i love you , i m not gonna crack\n",
    "\n",
    "### 2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가?\n",
    "\n",
    "- 특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었는가?\n",
    "\n",
    "- 데이터의 전처리: 입력된 문장을 정규 표현식을 이용해서 정제함으로써 특수 문자를 제거하였습니다.\n",
    "\n",
    "- 데이터셋 구성 과정: 우선, 모델 학습을 위하여 입력값(source)과 출력값(target)을 데이터셋 형식으로 정의한 후, 텐서플러우의 sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 8:2 분리하고, BUFFER_SIZE를 조정 함으로써 데이터셋을 구성하였습니다.\n",
    ".\n",
    "- 토크나이저 생성의 경우엔, 노드에 나온 설명대로 토큰 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하는 것을 권고하여서 maxlen=15로 설정하였으며, 패딩 처리를 위해서 padding='post'로 설정하였습니다.\n",
    "\n",
    "### 3. 텍스트 생성모델이 안정적으로 학습되었는가?\n",
    "\n",
    "- 텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?\n",
    "\n",
    "- batch size, embedding size, hidden size 등의 변수들을 조정해보면서 validation loss를 2.2 이하로 맞추기 위해서 다양한 시도를 진행하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계해 보았습니다.\n",
    "\n",
    "- 초기에는 변수들을 다양하게 조정을 하며 진행을 해보았으나, 제출 직전에는 다른 변수들은 고정해놓고 embedding size, hidden size 조정을 통해 loss 값을 조정해보았습니다.\n",
    "\n",
    "- 하지만, Total params가 늘어날수록 모델의 학습 시간이 오래 걸렸을 뿐만 아니라 loss 값이 유의미한 결과에 도달하지 못함을 확인 할 수 있었습니다.\n",
    "\n",
    "- 결과적으로, batch_size=256, embedding_size=512, hidden_size를 2048로 수정을 한 이후에서야, validation loss가 약 2.2 이하의 결과를 보이는 것을 확인 할 수 있었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-laser",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "### 1. 어려웠던 점\n",
    "\n",
    "- 노드를 따라서, 학습을 진행할 때, 처음 코드를 보고 이해하는 과정에서 어려움을 느꼈습니다.\n",
    "- 하지만, 여러 번 반복해서 읽어보면서 각 과정이 의미하는 바를 알아보는 좋은 시간이 되었다고 생각합니다.\n",
    "\n",
    "- 이번 노드에서 배운 대로 진행을 하였으나, 원하는 loss 값이 나오지 않아서 여러 시행착오를 겪어야 했던 것과 모델 학습 과정에서 많은 시간이 소요되는 과정에서 저의 인내심 부족으로 인해 어려움을 느꼈습니다.\n",
    "\n",
    "### 2. 배운 점\n",
    "\n",
    "- 하이퍼파라미터 설정 과정에서 다양한 시도를 해보고 최적의 결과를 내기 위해서 어떻게 해야 하는 지에 대해서 고민할 수 있었던 EXPLORATION이었다고 생각합니다.\n",
    "\n",
    "### 3. 마무리\n",
    "\n",
    "- 생각보다 모델의 학습시간을 위한 시간이 많이 소요된다는 생각이 듭니다. 좀 더 효율적으로 모델을 학습시키면서 높은 정확도를 내보고 싶다고 생각하였으나, 어떤 식으로 진행을 해야 좋은 방법인지에 대해서는 조금 더 AIFFEL 에서 학습을 하면서 이를 위해 필요한 것이 무엇이 있을 지 알아보는 시간이 필요할 것으로 보입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-pierce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
